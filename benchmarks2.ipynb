{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sigkernel2\n",
    "import sigkernel\n",
    "import csv\n",
    "import scipy\n",
    "import timeit\n",
    "import math\n",
    "device = torch.cuda.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(batch_size, length, dimension, device = torch.device('cpu')):\n",
    "  random_walks = torch.randn(batch_size, length, dimension, dtype = torch.double, device = device) / math.sqrt(length)\n",
    "  start = torch.zeros([batch_size, 1, dimension], device=device, dtype=torch.double)\n",
    "  random_walks = torch.cat((start, random_walks), dim=1)\n",
    "  random_walks = torch.cumsum(random_walks, dim=1)\n",
    "  return random_walks\n",
    "\n",
    "def median_distance(X, Y):\n",
    "    A = X.shape[0]\n",
    "    M = X.shape[1]\n",
    "    N = Y.shape[1]\n",
    "    Xs = torch.sum(X**2, dim=2)\n",
    "    Ys = torch.sum(Y**2, dim=2)\n",
    "    dist = -2.*torch.bmm(X, Y.permute(0,2,1))\n",
    "    dist += torch.reshape(Xs,(A,M,1)) + torch.reshape(Ys,(A,1,N))\n",
    "    return dist.view(A, -1).median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = generate(512, 50, 10, device = torch.device('cuda:0'))\n",
    "Y = generate(512, 50, 10, device = torch.device('cuda:0'))\n",
    "Z = generate(512, 50, 10, device = torch.device('cuda:0'))\n",
    "\n",
    "X = 1 * torch.sin(0.5 * Y) + X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_x = sigkernel2.SigKernel2(sigkernel2.RBFKernel(median_distance(X, X).cpu().item()), 1)\n",
    "sig_y = sigkernel2.SigKernel2(sigkernel2.RBFKernel(median_distance(Y, Y).cpu().item()), 1)\n",
    "sig_z = sigkernel2.SigKernel2(sigkernel2.RBFKernel(median_distance(Z, Z).cpu().item()), 1)\n",
    "sig_w = sigkernel2.SigKernel2(sigkernel2.RBFKernel(median_distance(Z, Z).cpu().item()), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "KX = sig_x.gram(X, max_batch=64)\n",
    "KY = sig_y.gram(Y, max_batch=64)\n",
    "KZ = sig_z.gram(Z, max_batch=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'critical_value': 6456.121145056668,\n",
       " 'gamma_p_value': 5.692797108986429e-109,\n",
       " 'perm_p_value': 0.0,\n",
       " 'mc_p_value': 0.0}"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = HSIC(KX, KY)\n",
    "test.test(perms=3000, max_batch=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HSIC:\n",
    "    def __init__(self, X, Y):\n",
    "        self.n = X.shape[0]\n",
    "        H = -torch.ones((self.n, self.n), device=X.device, dtype=X.dtype) / self.n\n",
    "        H[range(self.n), range(self.n)] = 1 - 1 / self.n\n",
    "        \n",
    "        self.X = X @ H\n",
    "        self.Y = Y @ H\n",
    "        \n",
    "    def test(self, alpha=0.05, perms=1000, max_batch=None):\n",
    "        critical_value = torch.einsum('ij,ji->', self.X, self.Y) / self.n\n",
    "        return {\n",
    "            \"critical_value\": critical_value.cpu().item(),\n",
    "            \"gamma_p_value\": self._gamma_approx(critical_value.cpu().item(), alpha),\n",
    "            \"perm_p_value\": self._perm(critical_value, alpha, perms, max_batch).cpu().item(),\n",
    "            \"mc_p_value\": self._montecarlo(critical_value, alpha, perms, max_batch).cpu().item()\n",
    "        }\n",
    "    \n",
    "    def _gamma_approx(self, value=None, alpha=0.05):\n",
    "        mu, var = self._empirical_moments()\n",
    "        loc = mu**2 / var\n",
    "        scale = var / mu\n",
    "        if value is None:\n",
    "            return scipy.stats.distributions.gamma.ppf(1 - alpha, a = loc.cpu(), scale = scale.cpu())\n",
    "        return scipy.stats.distributions.gamma.sf(value, a = loc.cpu(), scale = scale.cpu())\n",
    "    \n",
    "    def _empirical_moments(self):\n",
    "        mu_X = torch.trace(self.X)\n",
    "        mu_Y = torch.trace(self.Y)\n",
    "        \n",
    "        var_X = torch.einsum('ij,ji->', self.X, self.X)\n",
    "        var_Y = torch.einsum('ij,ji->', self.Y, self.Y)\n",
    "        \n",
    "        return mu_X * mu_Y / self.n**2, 2 * var_X * var_Y / self.n**4\n",
    "    \n",
    "    def _perm(self, value=None, alpha=0.05, m=100, max_batch=None):\n",
    "        max_batch = m if max_batch is None else max_batch\n",
    "        \n",
    "        mb_size = min(m, max_batch)\n",
    "        bm = -(-m // mb_size)\n",
    "        \n",
    "        stats = torch.zeros([m], device=self.X.device, dtype=self.X.dtype)\n",
    "        \n",
    "        for i in range(bm):\n",
    "            mb_size_i = m - mb_size * (bm - 1) if i == bm - 1 else mb_size\n",
    "            start = i * mb_size\n",
    "            stop = i * mb_size + mb_size_i\n",
    "            perms = torch.stack([torch.randperm(self.n, device=self.X.device) for _ in range(mb_size_i)])\n",
    "            Y_perm = torch.stack([self.Y[p][:, p] for p in perms])\n",
    "            tmp = torch.einsum('ij,kji->k', self.X, Y_perm) / self.n\n",
    "            stats[start:stop] = tmp\n",
    "            \n",
    "        if value is None:\n",
    "            return stats.abs().quantile(1 - alpha, 0)\n",
    "            \n",
    "        return (stats.abs() > value).double().mean()\n",
    "    \n",
    "    def _montecarlo(self, value=None, alpha=0.05, m=100, max_batch=None):\n",
    "        max_batch = m if max_batch is None else max_batch\n",
    "        \n",
    "        mb_size = min(m, max_batch)\n",
    "        bm = -(-m // mb_size)\n",
    "        \n",
    "        stats = torch.zeros([m], device=self.X.device, dtype=self.X.dtype)\n",
    "        eig_X = torch.linalg.eigvalsh(self.X).view(self.n, -1)\n",
    "        eig_Y = torch.linalg.eigvalsh(self.Y).view(-1, self.n)\n",
    "        \n",
    "        for i in range(bm):\n",
    "            mb_size_i = m - mb_size * (bm - 1) if i == bm - 1 else mb_size\n",
    "            start = i * mb_size\n",
    "            stop = i * mb_size + mb_size_i\n",
    "            z = torch.randn([mb_size_i, self.n, self.n], device=self.X.device, dtype=self.X.dtype).pow(2)\n",
    "            z = z * eig_X * eig_Y           \n",
    "            stats[start:stop] = z.mean(dim=[1,2])\n",
    "            \n",
    "        if value is None:\n",
    "            return stats.abs().quantile(1 - alpha, 0)\n",
    "            \n",
    "        return (stats.abs() > value).double().mean()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CHSIC:\n",
    "    def __init__(self, X, Y, Z):\n",
    "        self.n = X.shape[0]\n",
    "        self.X = self.sym(self.center_gram(X * Z))\n",
    "        self.Y = self.sym(self.center_gram(Y))\n",
    "        self.Z = self.sym(self.center_gram(Z))\n",
    "        \n",
    "    def sym(self, x):\n",
    "        return 0.5 * (x + x.T)\n",
    "    \n",
    "    def center_gram(self, x):\n",
    "        n = x.shape[0]\n",
    "        csums = x.sum(dim=0)\n",
    "        tsums = csums.sum()  \n",
    "        return x - (csums[None, :] + csums[:, None]) / n + (tsums / n ** 2)\n",
    "    \n",
    "    def eig_trunc(self, x):\n",
    "        vals, vecs = torch.linalg.eigh(x)\n",
    "        return vals, vecs\n",
    "\n",
    "    def eigvals_trunc(self, x):\n",
    "        vals = torch.linalg.eigvalsh(x)\n",
    "        return vals\n",
    "    \n",
    "    def fit_eps(self, Kx, Kz, maxit=10000, reltol=1e-3, lr=0.1):\n",
    "        vals, vecs = self.eig_trunc(Kx)\n",
    "        vals[vals < 0] = 0\n",
    "        vals = vals.sqrt()\n",
    "        vecs = 2 * math.sqrt(self.n) * vecs * vals / vals.max()\n",
    "        sig = torch.randn(1, requires_grad=True, device=Kx.device, dtype=Kx.dtype)\n",
    "        scale = torch.randn(1, requires_grad=True, device=Kx.device, dtype=Kx.dtype)\n",
    "        sig = torch.nn.Parameter(sig)\n",
    "        scale = torch.nn.Parameter(scale)\n",
    "        \n",
    "        optimizer = torch.optim.Adam([sig, scale], lr=lr)\n",
    "        \n",
    "        prev_sig = sig.detach().exp()\n",
    "        prev_scale = sig.detach().exp()\n",
    "        \n",
    "        for _ in range(maxit):\n",
    "            optimizer.zero_grad()\n",
    "            M = torch.exp(scale) * Kz + torch.exp(sig) * torch.eye(self.n, device=Kz.device, dtype=Kz.dtype)\n",
    "            dist = torch.distributions.MultivariateNormal(loc=0 * vecs.mean(dim=0), covariance_matrix=M)\n",
    "            loss = -dist.log_prob(vecs).mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            with torch.no_grad():               \n",
    "                if torch.abs(sig.exp() - prev_sig) / sig.exp() < reltol and torch.abs(scale.exp() - prev_scale) / scale.exp() < reltol:\n",
    "                    break\n",
    "                prev_sig = sig.detach().exp()\n",
    "                prev_scale = scale.detach().exp()\n",
    "                \n",
    "        return sig.detach().exp(), scale.detach().exp()\n",
    "        \n",
    "    \n",
    "    def test(self, m=1000, max_batch=None, alpha=0.05):\n",
    "        I = torch.eye(self.n, device=self.X.device, dtype=self.X.dtype)\n",
    "        x_eps, x_scale = self.fit_eps(self.X, self.Z, lr=2)\n",
    "        y_eps, y_scale = self.fit_eps(self.Y, self.Z, lr=2)\n",
    "        \n",
    "        Rx = x_eps * torch.linalg.pinv((x_scale * self.Z + x_eps * I), hermitian=True)\n",
    "        Ry = y_eps * torch.linalg.pinv((y_scale * self.Z + y_eps * I), hermitian=True)\n",
    "        \n",
    "        #R = I - self.Z @ torch.inverse(self.Z + eps * I)\n",
    "              \n",
    "        XZ = torch.einsum('ik,kl,jl->ij', Rx, self.X, Rx)\n",
    "        YY = torch.einsum('ik,kl,jl->ij', Ry, self.Y, Ry)\n",
    "        \n",
    "        critical_value = (XZ * YY).sum() / self.n\n",
    "        \n",
    "        return { \n",
    "            \"critical_value\": critical_value.cpu().item(),\n",
    "            \"mc_p_value\": self._null_dist(XZ, YY, critical_value, alpha, m, max_batch).cpu().item()\n",
    "        }\n",
    "        \n",
    "    \n",
    "    def _null_dist(self, KXZ, KYZ, value=None, alpha=0.05, m=1000, max_batch=None):\n",
    "        try:\n",
    "            val_x, eig_x = torch.linalg.eigh(KXZ)\n",
    "            val_x[val_x < 0] = 0\n",
    "        except:\n",
    "            val_x, eig_x = torch.linalg.eig(KXZ)\n",
    "            \n",
    "        try:\n",
    "            val_y, eig_y = torch.linalg.eigh(KYZ)\n",
    "            val_y[val_y < 0] = 0\n",
    "        except:\n",
    "              val_y, eig_y = torch.linalg.eig(KYZ)      \n",
    "        \n",
    "        vec_x = eig_x * val_x.sqrt()\n",
    "        vec_y = eig_y * val_y.sqrt()\n",
    "\n",
    "        ww = (vec_x.unsqueeze(2) * vec_y.unsqueeze(1)).reshape(self.n, -1)\n",
    "\n",
    "        ww_prod = ww @ ww.T if ww.shape[1] > self.n else ww.T @ ww\n",
    "        \n",
    "        try:\n",
    "            eig_vals = torch.linalg.eigvalsh(ww_prod)\n",
    "            eig_vals[eig_vals < 0] = 0\n",
    "        except:\n",
    "            eig_vals = torch.linalg.eigvals(ww_prod)\n",
    "        \n",
    "        max_batch = m if max_batch is None else max_batch\n",
    "        \n",
    "        mb_size = min(m, max_batch)\n",
    "        bm = -(-m // mb_size)\n",
    "        \n",
    "        stats = torch.zeros([m], device=self.X.device, dtype=self.X.dtype)\n",
    "        \n",
    "        for i in range(bm):\n",
    "            mb_size_i = m - mb_size * (bm - 1) if i == bm - 1 else mb_size\n",
    "            start = i * mb_size\n",
    "            stop = i * mb_size + mb_size_i\n",
    "            z = torch.randn([mb_size_i, self.n], device=self.X.device, dtype=self.X.dtype).pow(2)\n",
    "            z = z * eig_vals\n",
    "            stats[start:stop] = z.mean(dim=1)\n",
    "            \n",
    "        if value is None:\n",
    "            return stats.abs().quantile(1 - alpha, 0)\n",
    "            \n",
    "        return (stats.abs() > value).double().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = CHSIC(KX, KY, KZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'critical_value': 2402.2684856673623, 'mc_p_value': 0.428}"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.test(m=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "CX = test.X\n",
    "CY = test.Y\n",
    "CZ = test.Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class epsfinder():\n",
    "    def __init__(self, x, z):\n",
    "        self.eig_z = self.eigs(z)\n",
    "        self.eig_x = self.eigs(x)\n",
    "        self.K = x\n",
    "        self.device = x.device\n",
    "        self.dtype = x.dtype\n",
    "        self.n = x.shape[0]\n",
    "        self.sig = torch.nn.Parameter(torch.randn(1, requires_grad=True, device=self.device, dtype=self.dtype))\n",
    "        \n",
    "    def forward(self):\n",
    "        M = self.K + torch.exp(self.sig) * torch.eye(self.n, device=self.device, dtype=self.dtype)\n",
    "        dist = torch.distributions.MultivariateNormal(loc=self.eig_z.mean(dim=0), covariance_matrix=M)\n",
    "        return -dist.log_prob(self.eig_z.T).mean()\n",
    "        \n",
    "    def eigs(self, x):\n",
    "        val, vec = torch.linalg.eig(0.5 * (x + x.T))\n",
    "        return vec.real * val.real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = epsfinder(CZ, CY)\n",
    "optimizer = torch.optim.Adam([eps.sig], lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 505.2293297857577, Sig: 0.8823157194190312\n",
      "Loss: 482.0843100448312, Sig: 0.7983830489429063\n",
      "Loss: 459.07723777904215, Sig: 0.7224606618141663\n",
      "Loss: 436.2179810352863, Sig: 0.6537897531367733\n",
      "Loss: 413.5170949995638, Sig: 0.5916827461642138\n",
      "Loss: 390.9858776784415, Sig: 0.5355166762080917\n",
      "Loss: 368.6364299002704, Sig: 0.4847271842145996\n",
      "Loss: 346.4817196305238, Sig: 0.43880306401581815\n",
      "Loss: 324.53565049978613, Sig: 0.3972813124189331\n",
      "Loss: 302.81313431394403, Sig: 0.35974263597765466\n",
      "Loss: 281.3301671468446, Sig: 0.32580737253922\n",
      "Loss: 260.1039083986085, Sig: 0.2951317895167745\n",
      "Loss: 239.15276192983066, Sig: 0.2674047243367253\n",
      "Loss: 218.49645804398995, Sig: 0.24234453568674283\n",
      "Loss: 198.1561346770613, Sig: 0.2196963370723864\n",
      "Loss: 178.1544156524829, Sig: 0.19922948680600455\n",
      "Loss: 158.51548325761405, Sig: 0.18073531092519532\n",
      "Loss: 139.26514167978212, Sig: 0.16402503769200746\n",
      "Loss: 120.43086699101961, Sig: 0.14892792427840845\n",
      "Loss: 102.041838377277, Sig: 0.1352895580166842\n",
      "Loss: 84.12894416112505, Sig: 0.12297031620200226\n",
      "Loss: 66.72475486553279, Sig: 0.11184396989348336\n",
      "Loss: 49.86345412204692, Sig: 0.10179641848347737\n",
      "Loss: 33.580716671225076, Sig: 0.09272454300471464\n",
      "Loss: 2.899883377169112, Sig: 0.07714411662667632\n",
      "Loss: -11.4215040156155, Sig: 0.07047536604328407\n",
      "Loss: -25.011743672743528, Sig: 0.06446026796706046\n",
      "Loss: -37.8323318584134, Sig: 0.05903685372727138\n",
      "Loss: -49.84589962913842, Sig: 0.054149200831211004\n",
      "Loss: -61.01710931539756, Sig: 0.04974686013514738\n",
      "Loss: -71.31371433684768, Sig: 0.045784337107985816\n",
      "Loss: -80.70778246664746, Sig: 0.042220621923348224\n",
      "Loss: -89.17707113595317, Sig: 0.03901876355038864\n",
      "Loss: -96.70652760817401, Sig: 0.03614548340969267\n",
      "Loss: -103.28986661537648, Sig: 0.03357082452635556\n",
      "Loss: -108.93115370902476, Sig: 0.03126783245681281\n",
      "Loss: -113.64629546375792, Sig: 0.029212264598340556\n",
      "Loss: -117.46431042691043, Sig: 0.027382324818451056\n",
      "Loss: -120.42823153066242, Sig: 0.025758420670997345\n",
      "Loss: -122.59547731709296, Sig: 0.024322940797072746\n",
      "Loss: -124.03753253260079, Sig: 0.02306005043530414\n",
      "Loss: -124.83880503540587, Sig: 0.021955503273843133\n",
      "Loss: -125.09458010847811, Sig: 0.020996468145161536\n",
      "Loss: -124.9080754333961, Sig: 0.020171369273209543\n",
      "Loss: -124.38670401491382, Sig: 0.019469738915396974\n",
      "Loss: -123.63776482968116, Sig: 0.018882081298895053\n",
      "Loss: -122.76388244846866, Sig: 0.01839974675280583\n",
      "Loss: -121.85858561581838, Sig: 0.01801481492711793\n",
      "Loss: -120.26004682740879, Sig: 0.017508479079737915\n",
      "Loss: -119.67833276846935, Sig: 0.017373936658520817\n",
      "Loss: -119.28599146265283, Sig: 0.017310335598291857\n",
      "Loss: -119.09431552239533, Sig: 0.017311904101271317\n",
      "Loss: -119.09909113196132, Sig: 0.017373045883345664\n",
      "Loss: -119.28333468594653, Sig: 0.017488272657187158\n",
      "Loss: -119.6205328067373, Sig: 0.017652146585449825\n",
      "Loss: -120.07804752899997, Sig: 0.017859234532543882\n",
      "Loss: -120.62038388060566, Sig: 0.018104075902372705\n",
      "Loss: -121.21208182776381, Sig: 0.018381165563444227\n",
      "Loss: -121.82007352741657, Sig: 0.018684952847184536\n",
      "Loss: -122.41542648409408, Sig: 0.0190098569012929\n",
      "Loss: -122.97446344268147, Sig: 0.019350297849589165\n",
      "Loss: -123.47930467696114, Sig: 0.01970074232906236\n",
      "Loss: -123.91791555315706, Sig: 0.020055761125933297\n",
      "Loss: -124.2837625621037, Sig: 0.02041009589532221\n",
      "Loss: -124.57518698393073, Sig: 0.02075873139265221\n",
      "Loss: -124.79460042890771, Sig: 0.021096969320305952\n",
      "Loss: -124.9475942851064, Sig: 0.021420499828194363\n",
      "Loss: -125.04203884392702, Sig: 0.02172546690416515\n",
      "Loss: -125.08723019614425, Sig: 0.022008524327171593\n",
      "Loss: -125.09312581670025, Sig: 0.022266879489340658\n",
      "Loss: -125.06969433529011, Sig: 0.02249832316387784\n",
      "Loss: -125.02639200115283, Sig: 0.02270124413756351\n",
      "Loss: -124.91319333449496, Sig: 0.023018043950149072\n",
      "Loss: -124.85670167234183, Sig: 0.023131610935819204\n",
      "Loss: -124.80692905243417, Sig: 0.023215961438982025\n",
      "Loss: -124.76713521017041, Sig: 0.023272188532209904\n",
      "Loss: -124.73929059324938, Sig: 0.023301788504564484\n",
      "Loss: -124.72421253940703, Sig: 0.023306598190151174\n",
      "Loss: -124.72173531798418, Sig: 0.02328872983856736\n",
      "Loss: -124.73090007955307, Sig: 0.0232505057105187\n",
      "Loss: -124.75015238598345, Sig: 0.023194394317158955\n",
      "Loss: -124.77753675936866, Sig: 0.023122949905157907\n",
      "Loss: -124.81087951285193, Sig: 0.023038756448444563\n",
      "Loss: -124.84795295627544, Sig: 0.022944377065649346\n",
      "Loss: -124.88661586710813, Sig: 0.022842309458458075\n",
      "Loss: -124.92492685363612, Sig: 0.022734947674351542\n",
      "Loss: -124.96122888598904, Sig: 0.022624550246597968\n",
      "Loss: -124.99420480369076, Sig: 0.02251321455953868\n",
      "Loss: -125.02290499467034, Sig: 0.022402857129084606\n",
      "Loss: -125.04674964668371, Sig: 0.02229519937492425\n",
      "Loss: -125.06550896357709, Sig: 0.022191758388115657\n",
      "Loss: -125.07926548486327, Sig: 0.022093842160004745\n",
      "Loss: -125.08836312351278, Sig: 0.02200254872956442\n",
      "Loss: -125.09334773134472, Sig: 0.021918768719869957\n",
      "Loss: -125.09490391566202, Sig: 0.02184319076431197\n",
      "Loss: -125.09379248355418, Sig: 0.021776309363597982\n",
      "Loss: -125.08664973127568, Sig: 0.021669704467508048\n",
      "Loss: -125.08203750842989, Sig: 0.02163009612507676\n",
      "Loss: -125.07752487821075, Sig: 0.021599441414917313\n",
      "Loss: -125.07355883518267, Sig: 0.021577440780497587\n",
      "Loss: -125.07045636160025, Sig: 0.021563678740705024\n",
      "Loss: -125.06840644805922, Sig: 0.021557639600673868\n",
      "Loss: -125.06748028800223, Sig: 0.021558723378893845\n",
      "Loss: -125.06764769196408, Sig: 0.021566261778180714\n",
      "Loss: -125.06879762335214, Sig: 0.021579534032222847\n",
      "Loss: -125.07076078588963, Sig: 0.02159778246064909\n",
      "Loss: -125.07333236760988, Sig: 0.021620227565532172\n",
      "Loss: -125.07629333295628, Sig: 0.02164608250257125\n",
      "Loss: -125.07942901475239, Sig: 0.02167456676237283\n",
      "Loss: -125.08254415239654, Sig: 0.02170491890251796\n",
      "Loss: -125.08547391651406, Sig: 0.021736408180389948\n",
      "Loss: -125.08809082334884, Sig: 0.021768344950593113\n",
      "Loss: -125.09030775224126, Sig: 0.0218000897093684\n",
      "Loss: -125.09207752213592, Sig: 0.0218310606914645\n",
      "Loss: -125.09338965157373, Sig: 0.021860739951850512\n",
      "Loss: -125.09426502119807, Sig: 0.021888677894556986\n",
      "Loss: -125.09474918443969, Sig: 0.02191449624268327\n",
      "Loss: -125.09490504049352, Sig: 0.02193788947595415\n",
      "Loss: -125.0948055064431, Sig: 0.021958624793854572\n",
      "Loss: -125.09452671604811, Sig: 0.021976540692072496\n",
      "Loss: -125.09371792459436, Sig: 0.0220036073826599\n",
      "Loss: -125.09330948746234, Sig: 0.022012761863037193\n",
      "Loss: -125.09295954738201, Sig: 0.0220190938643993\n",
      "Loss: -125.09269734619559, Sig: 0.02202273761636497\n",
      "Loss: -125.09253900036143, Sig: 0.022023868701607066\n",
      "Loss: -125.09248873901866, Sig: 0.02202269705219539\n",
      "Loss: -125.09254079313843, Sig: 0.022019459830607328\n",
      "Loss: -125.09268168863096, Sig: 0.02201441435305616\n",
      "Loss: -125.09289270784257, Sig: 0.022007831198911084\n",
      "Loss: -125.0931523105094, Sig: 0.021999987633726854\n",
      "Loss: -125.09343834244117, Sig: 0.02199116145550428\n",
      "Loss: -125.09372990390706, Sig: 0.021981625354968167\n",
      "Loss: -125.09400879569915, Sig: 0.021971641861517472\n",
      "Loss: -125.09426050547063, Sig: 0.021961458927631367\n",
      "Loss: -125.09447473733852, Sig: 0.021951306186371495\n",
      "Loss: -125.09464552143902, Sig: 0.02194139189957316\n",
      "Loss: -125.09477096584186, Sig: 0.02193190059864326\n",
      "Loss: -125.09485273020366, Sig: 0.021922991405768442\n",
      "Loss: -125.09489530880825, Sig: 0.021914797010896794\n",
      "Loss: -125.09490521102097, Sig: 0.02190742326913883\n",
      "Loss: -125.09489012067183, Sig: 0.021900949374237717\n",
      "Loss: -125.09485810420412, Sig: 0.02189542855644515\n",
      "Loss: -125.09481692211176, Sig: 0.0218908892474423\n",
      "Loss: -125.09477348109077, Sig: 0.021887336650782477\n",
      "Loss: -125.094701022821, Sig: 0.02188310801405002\n",
      "Loss: -125.0946788810756, Sig: 0.02188234475849056\n",
      "Loss: -125.09466823112929, Sig: 0.021882398724107098\n",
      "Loss: -125.09466899217803, Sig: 0.02188319218394345\n",
      "Loss: -125.09468004052474, Sig: 0.021884638495010923\n",
      "Loss: -125.0946994982549, Sig: 0.021886644713728642\n",
      "Loss: -125.09472503265451, Sig: 0.021889114127711305\n",
      "Loss: -125.09475414007096, Sig: 0.021891948658226412\n",
      "Loss: -125.09478439345659, Sig: 0.021895051093493607\n",
      "Loss: -125.09481363949357, Sig: 0.02189832711925764\n",
      "Loss: -125.09484013759328, Sig: 0.02190168711960487\n",
      "Loss: -125.09486263931298, Sig: 0.021905047727676548\n",
      "Loss: -125.0948804118448, Sig: 0.02190833311262305\n",
      "Loss: -125.09489321332993, Sig: 0.0219114759957073\n",
      "Loss: -125.09490123046311, Sig: 0.021914418394770308\n",
      "Loss: -125.09490499019157, Sig: 0.021917112102195095\n",
      "Loss: -125.0949052575514, Sig: 0.021919518906935605\n",
      "Loss: -125.09490293072776, Sig: 0.02192161057601848\n",
      "Loss: -125.09489894282686, Sig: 0.0219233686151\n",
      "Loss: -125.09489417762015, Sig: 0.021924783831109462\n",
      "Loss: -125.09488940408158, Sig: 0.021925855722697146\n",
      "Loss: -125.09488523215202, Sig: 0.021926591726114025\n",
      "Loss: -125.09488208986713, Sig: 0.02192700634528708\n",
      "Loss: -125.0948802201548, Sig: 0.021927120195243927\n",
      "Loss: -125.09488043734092, Sig: 0.02192655248687265\n",
      "Loss: -125.09488226309965, Sig: 0.02192593346131522\n",
      "Loss: -125.0948849109335, Sig: 0.021925136657011585\n",
      "Loss: -125.09488808378906, Sig: 0.02192419781269284\n",
      "Loss: -125.09489148215928, Sig: 0.021923152737061535\n",
      "Loss: -125.09489483227313, Sig: 0.021922036463893988\n",
      "Loss: -125.0948979069131, Sig: 0.021920882498087244\n",
      "Loss: -125.0949005383187, Sig: 0.02191972216252375\n",
      "Loss: -125.09490262338295, Sig: 0.021918584052485965\n",
      "Loss: -125.09490412204542, Sig: 0.02191749360130914\n",
      "Loss: -125.09490505018783, Sig: 0.021916472758077087\n",
      "Loss: -125.09490546865817, Sig: 0.021915539775495478\n",
      "Loss: -125.09490547008916, Sig: 0.021914709103665338\n",
      "Loss: -125.0949051650922, Sig: 0.02191399138335951\n",
      "Loss: -125.09490466920627, Sig: 0.02191339353060253\n",
      "Loss: -125.09490409166621, Sig: 0.021912918902885454\n",
      "Loss: -125.09490352670807, Sig: 0.021912567536218973\n",
      "Loss: -125.09490304776787, Sig: 0.021912336441439646\n",
      "Loss: -125.09490270461484, Sig: 0.0219122199477273\n",
      "Loss: -125.09490252316121, Sig: 0.021912210081151497\n",
      "Loss: -125.09490250753328, Sig: 0.02191229696622084\n",
      "Loss: -125.09490264376367, Sig: 0.021912469238834862\n",
      "Loss: -125.09490290453591, Sig: 0.021912714459704368\n",
      "Loss: -125.09490325431986, Sig: 0.021913019518179318\n",
      "Loss: -125.09490406699629, Sig: 0.021913755633408964\n",
      "Loss: -125.09490445931812, Sig: 0.02191416044023638\n",
      "Loss: -125.09490480540524, Sig: 0.021914573198107114\n",
      "Loss: -125.09490508772498, Sig: 0.021914982598524733\n",
      "Loss: -125.09490529737033, Sig: 0.02191537846513527\n",
      "Loss: -125.09490543343509, Sig: 0.021915751908676718\n",
      "Loss: -125.09490550173015, Sig: 0.021916095436095538\n",
      "Loss: -125.09490551306456, Sig: 0.02191640301497911\n",
      "Loss: -125.09490548134974, Sig: 0.021916670095471522\n",
      "Loss: -125.09490542173062, Sig: 0.0219168935927248\n",
      "Loss: -125.09490534892072, Sig: 0.02191707183367651\n",
      "Loss: -125.09490527589048, Sig: 0.021917204472530476\n",
      "Loss: -125.09490521292487, Sig: 0.02191729237974666\n",
      "Loss: -125.09490516714307, Sig: 0.02191733750962046\n",
      "Loss: -125.09490514238448, Sig: 0.02191734275165575\n",
      "Loss: -125.0949051394534, Sig: 0.021917311770917745\n",
      "Loss: -125.0949051566091, Sig: 0.021917248842403664\n",
      "Loss: -125.09490519022145, Sig: 0.02191715868420435\n",
      "Loss: -125.09490523549191, Sig: 0.021917046293864227\n",
      "Loss: -125.09490528716961, Sig: 0.021916916791898347\n",
      "Loss: -125.09490534016763, Sig: 0.021916775275910825\n",
      "Loss: -125.09490539006711, Sig: 0.021916626688197625\n",
      "Loss: -125.0949054334489, Sig: 0.021916475699126886\n",
      "Loss: -125.09490546807886, Sig: 0.021916326607987955\n",
      "Loss: -125.09490550804313, Sig: 0.021916048996830185\n",
      "Loss: -125.09490551441638, Sig: 0.02191592659010339\n",
      "Loss: -125.094905513659, Sig: 0.021915818241538234\n",
      "Loss: -125.09490550776256, Sig: 0.02191572556458671\n",
      "Loss: -125.09490549882437, Sig: 0.021915649596723694\n",
      "Loss: -125.09490548881818, Sig: 0.021915590823891096\n",
      "Loss: -125.09490547942187, Sig: 0.021915549217591472\n",
      "Loss: -125.09490547189729, Sig: 0.021915524282545763\n",
      "Loss: -125.09490546704, Sig: 0.02191551511272357\n",
      "Loss: -125.0949054651905, Sig: 0.021915520453515375\n",
      "Loss: -125.09490546627174, Sig: 0.021915538767839103\n",
      "Loss: -125.09490546989375, Sig: 0.021915568304053695\n",
      "Loss: -125.09490547543962, Sig: 0.021915607163683005\n",
      "Loss: -125.09490548218082, Sig: 0.021915653367126565\n",
      "Loss: -125.09490548937202, Sig: 0.021915704915742164\n",
      "Loss: -125.09490549634265, Sig: 0.021915759848919594\n",
      "Loss: -125.09490550254925, Sig: 0.021915816295018105\n",
      "Loss: -125.09490550761052, Sig: 0.021915872515302397\n",
      "Loss: -125.09490551132922, Sig: 0.021915926940276824\n",
      "Loss: -125.09490551366974, Sig: 0.021915978198076454\n",
      "Loss: -125.09490551474235, Sig: 0.02191602513482119\n",
      "Loss: -125.09490551475992, Sig: 0.021916066827068245\n",
      "Loss: -125.09490551400435, Sig: 0.021916102586705465\n",
      "Loss: -125.09490551277639, Sig: 0.021916131958808052\n",
      "Loss: -125.0949055100281, Sig: 0.021916170830041215\n",
      "Loss: -125.09490550894967, Sig: 0.02191618048173938\n",
      "Loss: -125.09490550825146, Sig: 0.021916184009761776\n",
      "Loss: -125.09490550798674, Sig: 0.021916181899672354\n",
      "Loss: -125.09490550814598, Sig: 0.021916174753935883\n",
      "Loss: -125.0949055086703, Sig: 0.021916163263890415\n",
      "Loss: -125.09490550947075, Sig: 0.021916148181695627\n",
      "Loss: -125.09490551043584, Sig: 0.021916130293057896\n",
      "Loss: -125.09490551145792, Sig: 0.021916110391442447\n",
      "Loss: -125.09490551243766, Sig: 0.021916089254379857\n",
      "Loss: -125.09490551329623, Sig: 0.02191606762236251\n",
      "Loss: -125.09490551398301, Sig: 0.021916046180710014\n",
      "Loss: -125.09490551446943, Sig: 0.021916025544665284\n",
      "Loss: -125.09490551475677, Sig: 0.021916006247867367\n",
      "Loss: -125.09490551486397, Sig: 0.02191598873423789\n",
      "Loss: -125.09490551482631, Sig: 0.021915973353215924\n",
      "Loss: -125.09490551468778, Sig: 0.021915960358185115\n",
      "Loss: -125.09490551449274, Sig: 0.021915949907857793\n",
      "Loss: -125.0949055142864, Sig: 0.021915942070315043\n",
      "Loss: -125.09490551410002, Sig: 0.021915936829350457\n",
      "Loss: -125.09490551396254, Sig: 0.021915934092728236\n",
      "Loss: -125.09490551388569, Sig: 0.021915933701944074\n",
      "Loss: -125.09490551387412, Sig: 0.021915935443068437\n",
      "Loss: -125.09490551392345, Sig: 0.02191593905825648\n",
      "Loss: -125.09490551415576, Sig: 0.021915950730423916\n",
      "Loss: -125.09490551430491, Sig: 0.02191595815726317\n",
      "Loss: -125.09490551445441, Sig: 0.02191596621960217\n",
      "Loss: -125.09490551458995, Sig: 0.02191597460975157\n",
      "Loss: -125.0949055147027, Sig: 0.02191598303909171\n",
      "Loss: -125.09490551478709, Sig: 0.021915991245062685\n",
      "Loss: -125.09490551484049, Sig: 0.021915998996733545\n",
      "Loss: -125.09490551486476, Sig: 0.021916006098908966\n",
      "Loss: -125.09490551486365, Sig: 0.021916012394778944\n",
      "Loss: -125.09490551484652, Sig: 0.021916017767160657\n",
      "Loss: -125.09490551481872, Sig: 0.021916022138419814\n",
      "Loss: -125.09490551478636, Sig: 0.02191602546919107\n",
      "Loss: -125.09490551475707, Sig: 0.021916027756043125\n",
      "Loss: -125.09490551473405, Sig: 0.021916029028253525\n",
      "Loss: -125.09490551472086, Sig: 0.02191602934387098\n",
      "Loss: -125.09490551471657, Sig: 0.021916028785249144\n",
      "Loss: -125.094905514723, Sig: 0.021916027454236\n",
      "Loss: -125.09490551473779, Sig: 0.021916025467197665\n",
      "Loss: -125.09490551475729, Sig: 0.021916022950044602\n",
      "Loss: -125.09490551477919, Sig: 0.0219160200334139\n",
      "Loss: -125.09490551480272, Sig: 0.02191601684814276\n",
      "Loss: -125.09490551482506, Sig: 0.021916013521147717\n",
      "Loss: -125.0949055148411, Sig: 0.021916010171801618\n",
      "Loss: -125.09490551485487, Sig: 0.0219160069088767\n",
      "Loss: -125.09490551486574, Sig: 0.021916001010334592\n",
      "Loss: -125.09490551486655, Sig: 0.021915998520412202\n",
      "Loss: -125.09490551486334, Sig: 0.021915996406555657\n",
      "Loss: -125.09490551485827, Sig: 0.021915994700396058\n",
      "Loss: -125.09490551485383, Sig: 0.02191599341750769\n",
      "Loss: -125.09490551484865, Sig: 0.02191599255840506\n",
      "Loss: -125.09490551484541, Sig: 0.021915992109928208\n",
      "Loss: -125.0949055148441, Sig: 0.02191599204693741\n",
      "Loss: -125.09490551484318, Sig: 0.021915992334235926\n",
      "Loss: -125.09490551484453, Sig: 0.02191599292863892\n",
      "Loss: -125.09490551484654, Sig: 0.021915993781109616\n",
      "Loss: -125.09490551485104, Sig: 0.021915994838888127\n",
      "Loss: -125.09490551485365, Sig: 0.02191599604754569\n",
      "Loss: -125.09490551485733, Sig: 0.021915997352904764\n",
      "Loss: -125.09490551486073, Sig: 0.0219159987027754\n",
      "Loss: -125.09490551486273, Sig: 0.021916000048468404\n",
      "Loss: -125.09490551486519, Sig: 0.02191600134605641\n",
      "Loss: -125.09490551486644, Sig: 0.021916002557364683\n",
      "Loss: -125.0949055148667, Sig: 0.021916003650683868\n",
      "Loss: -125.09490551486601, Sig: 0.021916004601206438\n",
      "Loss: -125.0949055148654, Sig: 0.021916005391197507\n",
      "Loss: -125.09490551486527, Sig: 0.021916006009918287\n",
      "Loss: -125.09490551486377, Sig: 0.021916006453326954\n",
      "Loss: -125.09490551486317, Sig: 0.021916006723586755\n",
      "Loss: -125.09490551486286, Sig: 0.02191600678030696\n",
      "Loss: -125.09490551486354, Sig: 0.021916006595676742\n",
      "Loss: -125.09490551486367, Sig: 0.021916006293942206\n",
      "Loss: -125.0949055148641, Sig: 0.02191600589659785\n",
      "Loss: -125.0949055148639, Sig: 0.021916005426302315\n",
      "Loss: -125.09490551486442, Sig: 0.02191600490601113\n",
      "Loss: -125.09490551486594, Sig: 0.021916004358179246\n",
      "Loss: -125.09490551486512, Sig: 0.021916003804053542\n",
      "Loss: -125.09490551486553, Sig: 0.02191600326307084\n",
      "Loss: -125.09490551486653, Sig: 0.021916002752371985\n",
      "Loss: -125.09490551486614, Sig: 0.02191600228643782\n",
      "Loss: -125.09490551486616, Sig: 0.021916001876848213\n",
      "Loss: -125.09490551486644, Sig: 0.021916001532161167\n",
      "Loss: -125.09490551486664, Sig: 0.02191600125790519\n",
      "Loss: -125.09490551486661, Sig: 0.021916001056674978\n",
      "Loss: -125.09490551486552, Sig: 0.02191600092831781\n",
      "Loss: -125.09490551486613, Sig: 0.021916000870196208\n",
      "Loss: -125.09490551486573, Sig: 0.021916000877510975\n",
      "Loss: -125.09490551486611, Sig: 0.021916000943668232\n",
      "Loss: -125.09490551486672, Sig: 0.02191600106067398\n",
      "Loss: -125.0949055148655, Sig: 0.021916001219540262\n",
      "Loss: -125.0949055148663, Sig: 0.02191600141068787\n",
      "Loss: -125.09490551486601, Sig: 0.021916001624332396\n",
      "Loss: -125.09490551486552, Sig: 0.02191600185084163\n",
      "Loss: -125.09490551486726, Sig: 0.021916002306556073\n",
      "Loss: -125.09490551486614, Sig: 0.021916002519897084\n",
      "Loss: -125.09490551486635, Sig: 0.021916002714765345\n",
      "Loss: -125.09490551486643, Sig: 0.021916002886097675\n",
      "Loss: -125.09490551486604, Sig: 0.021916003030139804\n",
      "Loss: -125.09490551486687, Sig: 0.02191600314445473\n",
      "Loss: -125.09490551486662, Sig: 0.021916003227884437\n",
      "Loss: -125.09490551486661, Sig: 0.02191600328047043\n",
      "Loss: -125.09490551486724, Sig: 0.021916003303339527\n",
      "Loss: -125.09490551486618, Sig: 0.02191600329856223\n",
      "Loss: -125.09490551486553, Sig: 0.02191600326899095\n",
      "Loss: -125.09490551486692, Sig: 0.021916003218085702\n",
      "Loss: -125.09490551486655, Sig: 0.021916003149734485\n",
      "Loss: -125.0949055148666, Sig: 0.021916003068075156\n",
      "Loss: -125.09490551486614, Sig: 0.02191600297732493\n",
      "Loss: -125.09490551486681, Sig: 0.02191600288162285\n",
      "Loss: -125.09490551486647, Sig: 0.02191600278488962\n",
      "Loss: -125.09490551486724, Sig: 0.021916002690708144\n",
      "Loss: -125.09490551486692, Sig: 0.021916002602227196\n",
      "Loss: -125.09490551486685, Sig: 0.021916002522089462\n",
      "Loss: -125.09490551486599, Sig: 0.021916002452384356\n",
      "Loss: -125.09490551486778, Sig: 0.02191600239462498\n",
      "Loss: -125.09490551486654, Sig: 0.021916002349747936\n",
      "Loss: -125.09490551486634, Sig: 0.021916002318133822\n",
      "Loss: -125.09490551486715, Sig: 0.021916002293683463\n",
      "Loss: -125.09490551486668, Sig: 0.021916002299247925\n",
      "Loss: -125.09490551486678, Sig: 0.021916002315015934\n",
      "Loss: -125.09490551486724, Sig: 0.02191600233941839\n",
      "Loss: -125.09490551486672, Sig: 0.02191600237072069\n",
      "Loss: -125.0949055148662, Sig: 0.02191600240710158\n",
      "Loss: -125.09490551486638, Sig: 0.02191600244672786\n",
      "Loss: -125.0949055148668, Sig: 0.021916002487822606\n",
      "Loss: -125.09490551486634, Sig: 0.021916002528724977\n",
      "Loss: -125.09490551486593, Sig: 0.02191600256794025\n",
      "Loss: -125.09490551486633, Sig: 0.02191600260417909\n",
      "Loss: -125.09490551486667, Sig: 0.02191600263638568\n",
      "Loss: -125.09490551486638, Sig: 0.021916002663754642\n",
      "Loss: -125.09490551486576, Sig: 0.02191600268573716\n",
      "Loss: -125.09490551486658, Sig: 0.021916002702037057\n",
      "Loss: -125.09490551486635, Sig: 0.021916002712597967\n",
      "Loss: -125.09490551486726, Sig: 0.021916002717582764\n",
      "Loss: -125.0949055148667, Sig: 0.021916002717346825\n",
      "Loss: -125.09490551486718, Sig: 0.021916002712406624\n",
      "Loss: -125.09490551486675, Sig: 0.021916002703405265\n",
      "Loss: -125.09490551486655, Sig: 0.021916002691076478\n",
      "Loss: -125.09490551486626, Sig: 0.02191600267620863\n",
      "Loss: -125.09490551486746, Sig: 0.021916002659610006\n",
      "Loss: -125.09490551486749, Sig: 0.0219160026420766\n",
      "Loss: -125.09490551486675, Sig: 0.02191600260715966\n",
      "Loss: -125.0949055148663, Sig: 0.02191600259106944\n",
      "Loss: -125.09490551486624, Sig: 0.021916002576596556\n",
      "Loss: -125.0949055148663, Sig: 0.02191600256413511\n",
      "Loss: -125.09490551486678, Sig: 0.021916002553964947\n",
      "Loss: -125.09490551486715, Sig: 0.021916002546251815\n",
      "Loss: -125.09490551486621, Sig: 0.02191600254105186\n",
      "Loss: -125.09490551486661, Sig: 0.02191600253831986\n",
      "Loss: -125.09490551486724, Sig: 0.021916002537920542\n",
      "Loss: -125.09490551486697, Sig: 0.021916002539642227\n",
      "Loss: -125.0949055148666, Sig: 0.021916002543212208\n",
      "Loss: -125.09490551486708, Sig: 0.0219160025483129\n",
      "Loss: -125.09490551486647, Sig: 0.02191600255459832\n",
      "Loss: -125.09490551486687, Sig: 0.021916002561710043\n",
      "Loss: -125.09490551486641, Sig: 0.021916002569292145\n",
      "Loss: -125.09490551486655, Sig: 0.021916002577004694\n",
      "Loss: -125.09490551486607, Sig: 0.02191600258453536\n",
      "Loss: -125.09490551486658, Sig: 0.021916002591608842\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [56], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m25\u001b[39m:\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Sig: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00meps\u001b[38;5;241m.\u001b[39msig\u001b[38;5;241m.\u001b[39mexp()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_iterations = 100000\n",
    "\n",
    "with torch.no_grad():\n",
    "    sig_prev = eps.sig.clone()\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    optimizer.zero_grad()\n",
    "    loss = eps.forward()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 25:\n",
    "        print(f\"Loss: {loss.item()}, Sig: {eps.sig.exp().cpu().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn([512, 4], device=torch.device('cuda:0'), dtype=torch.double).unsqueeze(0)\n",
    "y = torch.randn([512, 4], device=torch.device('cuda:0'), dtype=torch.double).unsqueeze(0)\n",
    "z = torch.randn([512, 4], device=torch.device('cuda:0'), dtype=torch.double).unsqueeze(0)\n",
    "\n",
    "x = z**2 + x\n",
    "y = z**3 + y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_x = sigkernel2.RBFKernel(median_distance(x, x))\n",
    "rbf_y = sigkernel2.RBFKernel(median_distance(y, y))\n",
    "rbf_z = sigkernel2.RBFKernel(median_distance(z, z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "kx = rbf_x.batch_kernel(x, x).squeeze(0)\n",
    "ky = rbf_y.batch_kernel(y, y).squeeze(0)\n",
    "kz = rbf_z.batch_kernel(z, z).squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'critical_value': 6.769860165464699, 'mc_p_value': 0.0}"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = CHSIC(kx, kx, kz)\n",
    "test.test(m=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

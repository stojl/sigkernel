{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sigkernel2\n",
    "import sigkernel\n",
    "import csv\n",
    "import timeit\n",
    "import math\n",
    "device = torch.cuda.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(batch_size, length, dimension, device = torch.device('cpu')):\n",
    "  random_walks = torch.randn(batch_size, length, dimension, dtype = torch.double, device = device)\n",
    "  random_walks = torch.cumsum(random_walks, dim=1)\n",
    "  return random_walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = sigkernel2.SigKernel(sigkernel2.RBFKernel(1), 2)\n",
    "sig1 = sigkernel.SigKernel(sigkernel.RBFKernel(1), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/.local/lib/python3.9/site-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/home/paperspace/.local/lib/python3.9/site-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/home/paperspace/.local/lib/python3.9/site-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/home/paperspace/.local/lib/python3.9/site-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/home/paperspace/.local/lib/python3.9/site-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.3791e+32], device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Warm up to ensure JIT compilation\n",
    "X = generate(1, 128, 7, device = torch.device('cuda:0'))\n",
    "sig.compute_kernel(X, X, strided=False, alt_scheme=False)\n",
    "sig.compute_kernel(X, X, strided=True, alt_scheme=False)\n",
    "sig.compute_kernel(X, X, strided=False, alt_scheme=True)\n",
    "sig.compute_kernel(X, X, strided=True, alt_scheme=True)\n",
    "sig1.compute_kernel(X, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "dyadic_order = 0\n",
    "\n",
    "sig = sigkernel2.SigKernel(sigkernel2.RBFKernel(1), dyadic_order = 0)\n",
    "sig1 = sigkernel.SigKernel(sigkernel.RBFKernel(1), dyadic_order = 0)\n",
    "\n",
    "lengths = t = [16 * 2**i for i in range(6)]\n",
    "lengths.append(1023)\n",
    "\n",
    "execs = 100\n",
    "reps = 10\n",
    "\n",
    "with open(\"../speciale/rplots/bench_gpu_low.csv\", \"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Implementation\", \"Length\", \"Dyadic Order\", \"Run\", \"Result\"])\n",
    "      \n",
    "    for l in lengths:\n",
    "        \n",
    "        x = generate(100, l, 7, device = torch.device('cuda:0'))\n",
    "        \n",
    "        impl = {\n",
    "            \"Baseline\": lambda: sig1.compute_kernel(x, x),\n",
    "            \"Improved\": lambda: sig.compute_kernel(x, x, strided=False, alt_scheme=False)\n",
    "        }\n",
    "        \n",
    "        for name, func in impl.items():\n",
    "            # Use timeit with the function directly\n",
    "            timing_results = timeit.repeat(func, number=execs, repeat=reps)\n",
    "            \n",
    "            # Save results to CSV\n",
    "            for run, result in enumerate(timing_results, start=1):\n",
    "                writer.writerow([name, l, dyadic_order, run, result / execs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "dyadic_order = 1\n",
    "\n",
    "sig = sigkernel2.SigKernel(sigkernel2.RBFKernel(1), dyadic_order = 0)\n",
    "sig1 = sigkernel.SigKernel(sigkernel.RBFKernel(1), dyadic_order = 0)\n",
    "\n",
    "lengths = t = [16 * 2**i for i in range(5)]\n",
    "lengths.append(511)\n",
    "\n",
    "execs = 100\n",
    "reps = 10\n",
    "\n",
    "with open(\"../speciale/rplots/bench_gpu_low_1.csv\", \"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Implementation\", \"Length\", \"Dyadic Order\", \"Run\", \"Result\"])\n",
    "      \n",
    "    for l in lengths:\n",
    "        \n",
    "        x = generate(100, l, 7, device = torch.device('cuda:0'))\n",
    "        \n",
    "        impl = {\n",
    "            \"Baseline\": lambda: sig1.compute_kernel(x, x),\n",
    "            \"Improved\": lambda: sig.compute_kernel(x, x, strided=False, alt_scheme=False)\n",
    "        }\n",
    "        \n",
    "        for name, func in impl.items():\n",
    "            # Use timeit with the function directly\n",
    "            timing_results = timeit.repeat(func, number=execs, repeat=reps)\n",
    "            \n",
    "            # Save results to CSV\n",
    "            for run, result in enumerate(timing_results, start=1):\n",
    "                writer.writerow([name, l, dyadic_order, run, result / execs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cython_backend.sigkernel_cython() takes exactly one argument (2 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [32], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m sig1 \u001b[38;5;241m=\u001b[39m sigkernel\u001b[38;5;241m.\u001b[39mSigKernel(sigkernel\u001b[38;5;241m.\u001b[39mRBFKernel(\u001b[38;5;241m1\u001b[39m), dyadic_order \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     10\u001b[0m x \u001b[38;5;241m=\u001b[39m generate(\u001b[38;5;241m1\u001b[39m, l, \u001b[38;5;241m7\u001b[39m, device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m---> 12\u001b[0m \u001b[43msig1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m lengths \u001b[38;5;241m=\u001b[39m t \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m16\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mi \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m6\u001b[39m)]\n\u001b[1;32m     15\u001b[0m lengths\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m1023\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sigkernel/sigkernel.py:31\u001b[0m, in \u001b[0;36mSigKernel.compute_kernel\u001b[0;34m(self, X, Y, max_batch)\u001b[0m\n\u001b[1;32m     29\u001b[0m batch \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m max_batch:\n\u001b[0;32m---> 31\u001b[0m     K \u001b[38;5;241m=\u001b[39m \u001b[43m_SigKernel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatic_kernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdyadic_order\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_naive_solver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     cutoff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(batch\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sigkernel/sigkernel.py:241\u001b[0m, in \u001b[0;36m_SigKernel.forward\u001b[0;34m(ctx, X, Y, static_kernel, dyadic_order, _naive_solver)\u001b[0m\n\u001b[1;32m    237\u001b[0m     K \u001b[38;5;241m=\u001b[39m K[:,:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    239\u001b[0m \u001b[38;5;66;03m# if on CPU\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 241\u001b[0m     K \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43msigkernel_cython\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG_static_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_naive_solver\u001b[49m\u001b[43m)\u001b[49m, dtype\u001b[38;5;241m=\u001b[39mG_static\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mG_static\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    243\u001b[0m ctx\u001b[38;5;241m.\u001b[39msave_for_backward(X,Y,G_static,K)\n\u001b[1;32m    244\u001b[0m ctx\u001b[38;5;241m.\u001b[39mstatic_kernel \u001b[38;5;241m=\u001b[39m static_kernel\n",
      "\u001b[0;31mTypeError\u001b[0m: cython_backend.sigkernel_cython() takes exactly one argument (2 given)"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "dyadic_order = 0\n",
    "\n",
    "sig1 = sigkernel.SigKernel(sigkernel.RBFKernel(1), dyadic_order = 0)\n",
    "x = generate(1, l, 7, device = torch.device('cpu'))\n",
    "\n",
    "sig1.compute_kernel(x, x)\n",
    "\n",
    "lengths = t = [16 * 2**i for i in range(6)]\n",
    "lengths.append(1023)\n",
    "\n",
    "execs = 100\n",
    "reps = 10\n",
    "\n",
    "with open(\"../speciale/rplots/bench_gpu_cpu.csv\", \"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Implementation\", \"Length\", \"Dyadic Order\", \"Run\", \"Result\"])\n",
    "      \n",
    "    for l in lengths:\n",
    "        \n",
    "        x = generate(1, l, 7, device = torch.device('cuda:0'))\n",
    "        x_h = x.cpu()\n",
    "        \n",
    "        impl = {\n",
    "            \"Baseline - CPU\": lambda: sig1.compute_kernel(x_h, x_h),\n",
    "            \"Baseline - GPU\": lambda: sig1.compute_kernel(x, x)\n",
    "        }\n",
    "        \n",
    "        for name, func in impl.items():\n",
    "            # Use timeit with the function directly\n",
    "            timing_results = timeit.repeat(func, number=execs, repeat=reps)\n",
    "            \n",
    "            # Save results to CSV\n",
    "            for run, result in enumerate(timing_results, start=1):\n",
    "                writer.writerow([name, l, dyadic_order, run, result / execs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bench_memory(func):\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    func()\n",
    "    return torch.cuda.max_memory_allocated() / 1024 ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = [0, 1, 2, 3]\n",
    "\n",
    "with open(\"../speciale/rplots/bench_gpu_mem.csv\", \"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Implementation\", \"Length\", \"Dyadic Order\", \"Result\"])\n",
    "    \n",
    "    x = generate(100, 128, 7, device = torch.device('cuda:0'))\n",
    "    \n",
    "    for ord in orders:\n",
    "        sig = sigkernel2.SigKernel(sigkernel2.RBFKernel(1), ord)\n",
    "        sig1 = sigkernel.SigKernel(sigkernel.RBFKernel(1), ord)\n",
    "        \n",
    "        impl = {\n",
    "            \"Baseline\": lambda: sig1.compute_kernel(x, x),\n",
    "            \"Improved\": lambda: sig.compute_kernel(x, x, strided=False, alt_scheme=False)\n",
    "        }\n",
    "        \n",
    "        for name, func in impl.items():\n",
    "            # Use timeit with the function directly\n",
    "            result = bench_memory(func)\n",
    "            \n",
    "            writer.writerow([name, 128, ord, result]) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
